{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a67caa",
   "metadata": {},
   "source": [
    "# Training code for ParT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab0e1e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/z.ling.865/smartpixel-brevitas\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "print(sys.path[0])\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch_optimizer as optim\n",
    "from tqdm import tqdm\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import uproot\n",
    "from particle_transformer.model import ParTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf421d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the example file\n",
    "example_file = 'data/JetClass_Pythia_train_100M_part0/HToBB_000.root'\n",
    "\n",
    "# Load the content from the file\n",
    "tree = uproot.open(example_file)['tree']\n",
    "\n",
    "# Display the content of the \"tree\"\n",
    "tree.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0c9102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definition\n",
    "\n",
    "class ParticleDataset(Dataset): # 16 input features from particle data, turned into 17 derived features\n",
    "    def __init__(self, data_dir, in_features = 17, T=128, num_classes=10, tree_name=\"tree\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.in_features = in_features\n",
    "        self.T = T\n",
    "        self.num_classes = num_classes\n",
    "        self.tree_name = tree_name\n",
    "\n",
    "        # collect ROOT files\n",
    "        self.files = sorted(self.data_dir.glob(\"*.root\"))\n",
    "        assert len(self.files) > 0, f\"No .root files found in {data_dir}\"\n",
    "\n",
    "        # count events per file\n",
    "        self.file_event_counts = []\n",
    "        for f in self.files:\n",
    "            with uproot.open(f) as file:\n",
    "                tree = file[self.tree_name]\n",
    "                self.file_event_counts.append(tree.num_entries)\n",
    "\n",
    "        self.cum_events = np.cumsum(self.file_event_counts)\n",
    "\n",
    "        # Label order\n",
    "        self.label_names = [\n",
    "            \"label_QCD\",\n",
    "            \"label_Hbb\",\n",
    "            \"label_Hcc\",\n",
    "            \"label_Hgg\",\n",
    "            \"label_H4q\",\n",
    "            \"label_Hqql\",\n",
    "            \"label_Zqq\",\n",
    "            \"label_Wqq\",\n",
    "            \"label_Tbqq\",\n",
    "            \"label_Tbl\",\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.cum_events[-1])\n",
    "    \n",
    "    def _locate(self, idx):\n",
    "        file_idx = np.searchsorted(self.cum_events, idx, side=\"right\")\n",
    "        local_idx = idx if file_idx == 0 else idx - self.cum_events[file_idx - 1]\n",
    "        return file_idx, local_idx\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, local_idx = self._locate(idx)\n",
    "        file_path = self.files[file_idx]\n",
    "\n",
    "        with uproot.open(file_path) as file:\n",
    "            tree = file[self.tree_name]\n",
    "\n",
    "            # ---- load particle branches (jagged) ----\n",
    "            arr = tree.arrays(\n",
    "                [\n",
    "                    # kinematics\n",
    "                    \"part_px\",\n",
    "                    \"part_py\",\n",
    "                    \"part_pz\",\n",
    "                    \"part_energy\",\n",
    "                    \"part_deta\",\n",
    "                    \"part_dphi\",\n",
    "\n",
    "                    # displacement\n",
    "                    \"part_d0val\",\n",
    "                    \"part_d0err\",\n",
    "                    \"part_dzval\",\n",
    "                    \"part_dzerr\",\n",
    "\n",
    "                    # charge\n",
    "                    \"part_charge\",\n",
    "\n",
    "                    # PID (already one-hot)\n",
    "                    \"part_isChargedHadron\",\n",
    "                    \"part_isNeutralHadron\",\n",
    "                    \"part_isPhoton\",\n",
    "                    \"part_isElectron\",\n",
    "                    \"part_isMuon\",\n",
    "\n",
    "                    # jet-level\n",
    "                    \"jet_pt\",\n",
    "                    \"jet_energy\",\n",
    "                ],\n",
    "                entry_start=local_idx,\n",
    "                entry_stop=local_idx + 1,\n",
    "            )\n",
    "\n",
    "            # ---- load label ----\n",
    "            labels = tree.arrays(\n",
    "                self.label_names,\n",
    "                entry_start=local_idx,\n",
    "                entry_stop=local_idx + 1,\n",
    "            )[0]\n",
    "\n",
    "        # ---- particle arrays ----\n",
    "        px = arr[\"part_px\"][0]\n",
    "        py = arr[\"part_py\"][0]\n",
    "        pz = arr[\"part_pz\"][0]\n",
    "        E  = arr[\"part_energy\"][0]\n",
    "        deta = arr[\"part_deta\"][0]\n",
    "        dphi = arr[\"part_dphi\"][0]\n",
    "\n",
    "        # ---- derived features ----\n",
    "        pt = np.sqrt(px**2 + py**2) + 1e-8\n",
    "        log_pt = np.log(pt)\n",
    "        log_E = np.log(E + 1e-8)\n",
    "\n",
    "        jet_pt = arr[\"jet_pt\"][0]\n",
    "        jet_E  = arr[\"jet_energy\"][0]\n",
    "\n",
    "        log_pt_rel = np.log(pt / (jet_pt + 1e-8))\n",
    "        log_E_rel  = np.log(E / (jet_E + 1e-8))\n",
    "\n",
    "        deltaR = np.sqrt(deta**2 + dphi**2)\n",
    "\n",
    "\n",
    "        # stack particle features\n",
    "        feats = np.stack(\n",
    "            [\n",
    "                ak.to_numpy(deta),\n",
    "                ak.to_numpy(dphi),\n",
    "                ak.to_numpy(log_pt),\n",
    "                ak.to_numpy(log_E),\n",
    "                ak.to_numpy(log_pt_rel),\n",
    "                ak.to_numpy(log_E_rel),\n",
    "                ak.to_numpy(deltaR),\n",
    "                ak.to_numpy(arr[\"part_d0val\"][0]),\n",
    "                ak.to_numpy(arr[\"part_d0err\"][0]),\n",
    "                ak.to_numpy(arr[\"part_dzval\"][0]),\n",
    "                ak.to_numpy(arr[\"part_dzerr\"][0]),\n",
    "                ak.to_numpy(arr[\"part_charge\"][0]),\n",
    "                ak.to_numpy(arr[\"part_isChargedHadron\"][0]),\n",
    "                ak.to_numpy(arr[\"part_isNeutralHadron\"][0]),\n",
    "                ak.to_numpy(arr[\"part_isPhoton\"][0]),\n",
    "                ak.to_numpy(arr[\"part_isElectron\"][0]),\n",
    "                ak.to_numpy(arr[\"part_isMuon\"][0]),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # feats = ak.to_numpy(feats)\n",
    "        n_particles = min(len(feats), self.T)\n",
    "\n",
    "        # ---- build tensors ----\n",
    "        x = torch.zeros(self.T, self.in_features, dtype=torch.float32)\n",
    "        x[:n_particles] = torch.from_numpy(feats[:n_particles]).float()\n",
    "\n",
    "        assert x.dim() == 2\n",
    "        assert x.shape[-1] == self.in_features   # 17\n",
    "\n",
    "        mask = torch.zeros(self.T, dtype=torch.bool)\n",
    "        mask[:n_particles] = True\n",
    "\n",
    "\n",
    "        # pairwise interaction tensor U (T, T, 4)\n",
    "        U = torch.zeros(self.T, self.T, 4, dtype=torch.float32) # 4 features per pair\n",
    "\n",
    "        for i in range(n_particles):\n",
    "            for j in range(n_particles):\n",
    "                delta_eta = deta[i] - deta[j]\n",
    "                delta_phi = dphi[i] - dphi[j]\n",
    "                delta = np.sqrt(delta_eta**2 + delta_phi**2)\n",
    "\n",
    "                kT = min(pt[i], pt[j]) * delta\n",
    "                z = min(pt[i], pt[j]) / (pt[i] + pt[j] + 1e-8)\n",
    "                m2 = (E[i] + E[j])**2 - ((px[i] + px[j])**2 + (py[i] + py[j])**2 + (pz[i] + pz[j])**2)\n",
    "\n",
    "                U[i, j, 0] = float(delta)\n",
    "                U[i, j, 1] = float(kT)\n",
    "                U[i, j, 2] = float(z)\n",
    "                U[i, j, 3] = float(m2)\n",
    "\n",
    "        # ---- build single class label ----\n",
    "        label_vals = [labels[name] for name in self.label_names]\n",
    "\n",
    "        # QCD is float, others are bool/int\n",
    "        if label_vals[0] > 0:\n",
    "            y = 0\n",
    "        else:\n",
    "            y = int(np.argmax(label_vals[1:]) + 1)\n",
    "\n",
    "\n",
    "        return x, U, mask, y\n",
    "    \n",
    "# Training and evaluation functions\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "def train_one_iter(model, batch, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    x, U, mask, y = batch\n",
    "    x = x.to(device)\n",
    "    U = U.to(device)\n",
    "    mask = mask.to(device)\n",
    "    mask_bool = mask\n",
    "    attn_mask = (~mask_bool).float() * (-1e9)\n",
    "    attn_mask = attn_mask[:, None, None, :]   # [B, 1, 1, N]\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # print(\"x:\", getattr(x, \"shape\", None))\n",
    "    # print(\"U:\", getattr(U, \"shape\", None))\n",
    "    # print(\"mask:\", getattr(mask, \"shape\", None))\n",
    "    # print(\"attn_mask:\", getattr(attn_mask, \"shape\", None))\n",
    "    logits = model(x, U, attn_mask)\n",
    "    if hasattr(logits, \"value\"):   # unwrap Brevitas QuantTensor to a torch.Tensor\n",
    "        logits = logits.value \n",
    "\n",
    "    logits = logits.squeeze(1)     # [B, 1, 10] -> [B, 10]\n",
    "    # print(\"DEBUG logits type:\", type(logits), \"shape:\", getattr(logits, \"shape\", None))\n",
    "    # print(\"DEBUG y shape:\", y.shape, \"dtype:\", y.dtype)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, U, mask, y in loader:\n",
    "        x = x.to(device)\n",
    "        U = U.to(device)\n",
    "        mask = mask.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(x, U, mask)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.numel()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0036ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z.ling.865/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/_tensor.py:1645: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1939.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.52 GiB of which 16.69 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 68.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, total_iters \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     65\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_iter)\n\u001b[0;32m---> 67\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# Evaluation & checkpoint\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 207\u001b[0m, in \u001b[0;36mtrain_one_iter\u001b[0;34m(model, batch, optimizer, device)\u001b[0m\n\u001b[1;32m    201\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# print(\"x:\", getattr(x, \"shape\", None))\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# print(\"U:\", getattr(U, \"shape\", None))\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# print(\"mask:\", getattr(mask, \"shape\", None))\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# print(\"attn_mask:\", getattr(attn_mask, \"shape\", None))\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m):   \u001b[38;5;66;03m# unwrap Brevitas QuantTensor to a torch.Tensor\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mvalue \n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/smartpixel-brevitas/particle_transformer/model.py:78\u001b[0m, in \u001b[0;36mParTModel.forward\u001b[0;34m(self, x, U, mask)\u001b[0m\n\u001b[1;32m     76\u001b[0m U \u001b[38;5;241m=\u001b[39m U\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpab_num):\n\u001b[0;32m---> 78\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpab_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Initial input for Class Attention Blocks\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/smartpixel-brevitas/particle_transformer/blocks/particle_attention.py:65\u001b[0m, in \u001b[0;36mQuantParticleAttentionBlock.forward\u001b[0;34m(self, x, U, attn_mask)\u001b[0m\n\u001b[1;32m     62\u001b[0m x_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Quantized P-MHA\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# [B, N, D]\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attn_out, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEBUG pmha tuple lens/shapes:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28mlen\u001b[39m(attn_out),\n\u001b[1;32m     69\u001b[0m         [\u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m attn_out])\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/smartpixel-brevitas/particle_transformer/layers/mha.py:226\u001b[0m, in \u001b[0;36mQPMHALayer.forward\u001b[0;34m(self, x, U, attn_mask, key_value)\u001b[0m\n\u001b[1;32m    223\u001b[0m     key_value \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    224\u001b[0m _, N_kv, _ \u001b[38;5;241m=\u001b[39m key_value\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 226\u001b[0m Q, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m _, K, V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv_proj(key_value)\n\u001b[1;32m    229\u001b[0m Qh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_heads(Q)            \u001b[38;5;66;03m# [B,H,N,Hd]\u001b[39;00m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/smartpixel-brevitas/particle_transformer/layers/mha.py:35\u001b[0m, in \u001b[0;36mQuantQKVProj.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     31\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m     32\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# x: [B, N, D]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     x_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_in(x)\n\u001b[0;32m---> 35\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_q\u001b[49m\u001b[43m)\u001b[49m            \u001b[38;5;66;03m# [B, N, 3D]\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_qkv(qkv)\n\u001b[1;32m     37\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# each [B, N, D]\u001b[39;00m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/smartpixel-brevitas/particle_transformer/layers/dense.py:51\u001b[0m, in \u001b[0;36mQDenseLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Brevitas expects float tensors during QAT; it will insert fake quant.\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/brevitas/nn/quant_linear.py:68\u001b[0m, in \u001b[0;36mQuantLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[Tensor, QuantTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, QuantTensor]:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/brevitas/nn/quant_layer.py:158\u001b[0m, in \u001b[0;36mQuantWeightBiasInputOutputLayer.forward_impl\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     quant_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m quant_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_quant(output_tensor)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpack_output(quant_output)\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/brevitas/nn/quant_linear.py:71\u001b[0m, in \u001b[0;36mQuantLinear.inner_forward_impl\u001b[0;34m(self, x, quant_weight, quant_bias)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, quant_weight: Tensor, quant_bias: Optional[Tensor]):\n\u001b[0;32m---> 71\u001b[0m     output_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_tensor\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/brevitas/quant_tensor/int_quant_tensor.py:49\u001b[0m, in \u001b[0;36mIntQuantTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m INT_QUANT_TENSOR_FN_HANDLER:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mINT_QUANT_TENSOR_FN_HANDLER\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m QUANT_TENSOR_FN_HANDLER:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QUANT_TENSOR_FN_HANDLER[func](\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/brevitas/quant_tensor/int_torch_handler.py:75\u001b[0m, in \u001b[0;36mlinear_handler\u001b[0;34m(quant_input, quant_weight, bias, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@implements_int_qt\u001b[39m(F\u001b[38;5;241m.\u001b[39mlinear)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlinear_handler\u001b[39m(quant_input, quant_weight, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 75\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mquant_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/brevitas/quant_tensor/int_torch_handler.py:169\u001b[0m, in \u001b[0;36mquant_layer\u001b[0;34m(fn, quant_input, quant_weight, bias, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\n\u001b[1;32m    163\u001b[0m         _unpack_quant_tensor(quant_input),\n\u001b[1;32m    164\u001b[0m         _unpack_quant_tensor(quant_weight),\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quant_input, IntQuantTensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quant_weight, IntQuantTensor):\n\u001b[1;32m    177\u001b[0m     output_bit_width \u001b[38;5;241m=\u001b[39m max_acc_bit_width(\n\u001b[1;32m    178\u001b[0m         quant_input\u001b[38;5;241m.\u001b[39mbit_width,\n\u001b[1;32m    179\u001b[0m         quant_weight\u001b[38;5;241m.\u001b[39mbit_width,\n\u001b[1;32m    180\u001b[0m         quant_weight\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.52 GiB of which 16.69 MiB is free. Including non-PyTorch memory, this process has 23.50 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 68.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Main training cell\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 256 # 512 is the original setting\n",
    "total_iters = 1000000\n",
    "eval_interval = 20000\n",
    "initial_lr = 1e-3\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_ds = ParticleDataset(data_dir=\"data/JetClass_Pythia_train_100M_part0\", num_classes=num_classes)\n",
    "val_ds   = ParticleDataset(data_dir=\"data/JetClass_Pythia_val_5M/val_5M\",  num_classes=num_classes)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "model = ParTModel(\n",
    "    in_features=17,\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    num_classes=num_classes,\n",
    "    w_bit_width=8,\n",
    "    a_bit_width=8,\n",
    "    pab_num=8,\n",
    "    cab_num=2,\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "base_optimizer = optim.RAdam(\n",
    "    model.parameters(),\n",
    "    lr=initial_lr,\n",
    "    betas=(0.95, 0.999),\n",
    "    eps=1e-5,\n",
    "    weight_decay=0.0\n",
    ")\n",
    "\n",
    "optimizer = optim.Lookahead(\n",
    "    base_optimizer,\n",
    "    k=6,\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "def lr_lambda(step):\n",
    "    warm = int(0.7 * total_iters)\n",
    "    if step < warm:\n",
    "        return 1.0\n",
    "    decay_steps = (step - warm) // 20000\n",
    "    return 0.99 ** decay_steps  # decays to ~1% by end\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lr_lambda=lr_lambda\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "\n",
    "best_val_acc = 0.0\n",
    "global_step = 0\n",
    "\n",
    "train_iter = cycle(train_loader)\n",
    "\n",
    "for step in range(1, total_iters + 1):\n",
    "    batch = next(train_iter)\n",
    "\n",
    "    loss = train_one_iter(\n",
    "        model=model,\n",
    "        batch=batch,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluation & checkpoint\n",
    "    if step % eval_interval == 0:\n",
    "        val_acc = evaluate(model, val_loader, device)\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        print(\n",
    "            f\"Iter {step:7d} | \"\n",
    "            f\"Loss {loss:.4f} | \"\n",
    "            f\"Val Acc {val_acc:.4f} | \"\n",
    "            f\"LR {lr:.2e}\"\n",
    "        )\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"results/best_part_model.pth\")\n",
    "\n",
    "\n",
    "# Final checkpoint\n",
    "torch.save(model.state_dict(), \"results/part_model_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad16908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "test_ds = ParticleDataset(\n",
    "    data_dir=\"data/test_20M\",\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_acc = evaluate(model, test_loader, device)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
