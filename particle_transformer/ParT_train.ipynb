{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a67caa",
   "metadata": {},
   "source": [
    "# Training code for ParT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab0e1e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/z.ling.865/smartpixel-brevitas\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "print(sys.path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab2b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch_optimizer as optim\n",
    "from tqdm import tqdm\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import uproot\n",
    "from particle_transformer.model import ParTModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcf421d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 | typename                 | interpretation                \n",
      "---------------------+--------------------------+-------------------------------\n",
      "part_px              | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "part_py              | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "part_pz              | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "part_energy          | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "part_deta            | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "part_dphi            | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "part_d0val           | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "part_d0err           | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "part_dzval           | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "part_dzerr           | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "part_charge          | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "part_isChargedHadron | std::vector<int32_t>     | AsJagged(AsDtype('>i4'), he...\n",
      "part_isNeutralHadron | std::vector<int32_t>     | AsJagged(AsDtype('>i4'), he...\n",
      "part_isPhoton        | std::vector<int32_t>     | AsJagged(AsDtype('>i4'), he...\n",
      "part_isElectron      | std::vector<int32_t>     | AsJagged(AsDtype('>i4'), he...\n",
      "part_isMuon          | std::vector<int32_t>     | AsJagged(AsDtype('>i4'), he...\n",
      "label_QCD            | float                    | AsDtype('>f4')\n",
      "label_Hbb            | bool                     | AsDtype('bool')\n",
      "label_Hcc            | bool                     | AsDtype('bool')\n",
      "label_Hgg            | bool                     | AsDtype('bool')\n",
      "label_H4q            | bool                     | AsDtype('bool')\n",
      "label_Hqql           | bool                     | AsDtype('bool')\n",
      "label_Zqq            | int32_t                  | AsDtype('>i4')\n",
      "label_Wqq            | int32_t                  | AsDtype('>i4')\n",
      "label_Tbqq           | int32_t                  | AsDtype('>i4')\n",
      "label_Tbl            | int32_t                  | AsDtype('>i4')\n",
      "jet_pt               | float                    | AsDtype('>f4')\n",
      "jet_eta              | float                    | AsDtype('>f4')\n",
      "jet_phi              | float                    | AsDtype('>f4')\n",
      "jet_energy           | float                    | AsDtype('>f4')\n",
      "jet_nparticles       | float                    | AsDtype('>f4')\n",
      "jet_sdmass           | float                    | AsDtype('>f4')\n",
      "jet_tau1             | float                    | AsDtype('>f4')\n",
      "jet_tau2             | float                    | AsDtype('>f4')\n",
      "jet_tau3             | float                    | AsDtype('>f4')\n",
      "jet_tau4             | float                    | AsDtype('>f4')\n",
      "aux_genpart_eta      | float                    | AsDtype('>f4')\n",
      "aux_genpart_phi      | float                    | AsDtype('>f4')\n",
      "aux_genpart_pid      | float                    | AsDtype('>f4')\n",
      "aux_genpart_pt       | float                    | AsDtype('>f4')\n",
      "aux_truth_match      | float                    | AsDtype('>f4')\n"
     ]
    }
   ],
   "source": [
    "# Download the example file\n",
    "example_file = 'data/JetClass_Pythia_train_100M_part0/HToBB_000.root'\n",
    "\n",
    "# Load the content from the file\n",
    "tree = uproot.open(example_file)['tree']\n",
    "\n",
    "# Display the content of the \"tree\"\n",
    "tree.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f0c9102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definition\n",
    "\n",
    "class ParticleDataset(Dataset): # 16 input features from particle data, turned into 17 derived features\n",
    "    def __init__(self, data_dir, in_features = 17, T=128, num_classes=10, tree_name=\"tree\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.in_features = in_features\n",
    "        self.T = T\n",
    "        self.num_classes = num_classes\n",
    "        self.tree_name = tree_name\n",
    "\n",
    "        # collect ROOT files\n",
    "        self.files = sorted(self.data_dir.glob(\"*.root\"))\n",
    "        assert len(self.files) > 0, f\"No .root files found in {data_dir}\"\n",
    "\n",
    "        # count events per file\n",
    "        self.file_event_counts = []\n",
    "        for f in self.files:\n",
    "            with uproot.open(f) as file:\n",
    "                tree = file[self.tree_name]\n",
    "                self.file_event_counts.append(tree.num_entries)\n",
    "\n",
    "        self.cum_events = np.cumsum(self.file_event_counts)\n",
    "\n",
    "        # Label order\n",
    "        self.label_names = [\n",
    "            \"label_QCD\",\n",
    "            \"label_Hbb\",\n",
    "            \"label_Hcc\",\n",
    "            \"label_Hgg\",\n",
    "            \"label_H4q\",\n",
    "            \"label_Hqql\",\n",
    "            \"label_Zqq\",\n",
    "            \"label_Wqq\",\n",
    "            \"label_Tbqq\",\n",
    "            \"label_Tbl\",\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.cum_events[-1])\n",
    "    \n",
    "    def _locate(self, idx):\n",
    "        file_idx = np.searchsorted(self.cum_events, idx, side=\"right\")\n",
    "        local_idx = idx if file_idx == 0 else idx - self.cum_events[file_idx - 1]\n",
    "        return file_idx, local_idx\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, local_idx = self._locate(idx)\n",
    "        file_path = self.files[file_idx]\n",
    "\n",
    "        with uproot.open(file_path) as file:\n",
    "            tree = file[self.tree_name]\n",
    "\n",
    "            # ---- load particle branches (jagged) ----\n",
    "            arr = tree.arrays(\n",
    "                [\n",
    "                    # kinematics\n",
    "                    \"part_px\",\n",
    "                    \"part_py\",\n",
    "                    \"part_pz\",\n",
    "                    \"part_energy\",\n",
    "                    \"part_deta\",\n",
    "                    \"part_dphi\",\n",
    "\n",
    "                    # displacement\n",
    "                    \"part_d0val\",\n",
    "                    \"part_d0err\",\n",
    "                    \"part_dzval\",\n",
    "                    \"part_dzerr\",\n",
    "\n",
    "                    # charge\n",
    "                    \"part_charge\",\n",
    "\n",
    "                    # PID (already one-hot)\n",
    "                    \"part_isChargedHadron\",\n",
    "                    \"part_isNeutralHadron\",\n",
    "                    \"part_isPhoton\",\n",
    "                    \"part_isElectron\",\n",
    "                    \"part_isMuon\",\n",
    "\n",
    "                    # jet-level\n",
    "                    \"jet_pt\",\n",
    "                    \"jet_energy\",\n",
    "                ],\n",
    "                entry_start=local_idx,\n",
    "                entry_stop=local_idx + 1,\n",
    "            )\n",
    "\n",
    "            # ---- load label ----\n",
    "            labels = tree.arrays(\n",
    "                self.label_names,\n",
    "                entry_start=local_idx,\n",
    "                entry_stop=local_idx + 1,\n",
    "            )[0]\n",
    "\n",
    "        # ---- particle arrays ----\n",
    "        px = arr[\"part_px\"][0]\n",
    "        py = arr[\"part_py\"][0]\n",
    "        pz = arr[\"part_pz\"][0]\n",
    "        E  = arr[\"part_energy\"][0]\n",
    "        deta = arr[\"part_deta\"][0]\n",
    "        dphi = arr[\"part_dphi\"][0]\n",
    "\n",
    "        # ---- derived features ----\n",
    "        pt = np.sqrt(px**2 + py**2) + 1e-8\n",
    "        log_pt = np.log(pt)\n",
    "        log_E = np.log(E + 1e-8)\n",
    "\n",
    "        jet_pt = arr[\"jet_pt\"][0]\n",
    "        jet_E  = arr[\"jet_energy\"][0]\n",
    "\n",
    "        log_pt_rel = np.log(pt / (jet_pt + 1e-8))\n",
    "        log_E_rel  = np.log(E / (jet_E + 1e-8))\n",
    "\n",
    "        deltaR = np.sqrt(deta**2 + dphi**2)\n",
    "\n",
    "\n",
    "        # stack particle features\n",
    "        feats = np.stack(\n",
    "            [\n",
    "                ak.to_numpy(deta),\n",
    "                ak.to_numpy(dphi),\n",
    "                ak.to_numpy(log_pt),\n",
    "                ak.to_numpy(log_E),\n",
    "                ak.to_numpy(log_pt_rel),\n",
    "                ak.to_numpy(log_E_rel),\n",
    "                ak.to_numpy(deltaR),\n",
    "                ak.to_numpy(arr[\"part_d0val\"][0]),\n",
    "                ak.to_numpy(arr[\"part_d0err\"][0]),\n",
    "                ak.to_numpy(arr[\"part_dzval\"][0]),\n",
    "                ak.to_numpy(arr[\"part_dzerr\"][0]),\n",
    "                ak.to_numpy(arr[\"part_charge\"][0]),\n",
    "                ak.to_numpy(arr[\"part_isChargedHadron\"][0]),\n",
    "                ak.to_numpy(arr[\"part_isNeutralHadron\"][0]),\n",
    "                ak.to_numpy(arr[\"part_isPhoton\"][0]),\n",
    "                ak.to_numpy(arr[\"part_isElectron\"][0]),\n",
    "                ak.to_numpy(arr[\"part_isMuon\"][0]),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # feats = ak.to_numpy(feats)\n",
    "        n_particles = min(len(feats), self.T)\n",
    "\n",
    "        # ---- build tensors ----\n",
    "        x = torch.zeros(self.T, self.in_features, dtype=torch.float32)\n",
    "        x[:n_particles] = torch.from_numpy(feats[:n_particles]).float()\n",
    "\n",
    "        assert x.dim() == 2\n",
    "        assert x.shape[-1] == self.in_features   # 17\n",
    "\n",
    "        mask = torch.zeros(self.T, dtype=torch.bool)\n",
    "        mask[:n_particles] = True\n",
    "\n",
    "        # pairwise interaction tensor U (T, T, 4)\n",
    "        U = torch.zeros(self.T, self.T, 4, dtype=torch.float32) # 4 features per pair\n",
    "\n",
    "        for i in range(n_particles):\n",
    "            for j in range(n_particles):\n",
    "                delta_eta = deta[i] - deta[j]\n",
    "                delta_phi = dphi[i] - dphi[j]\n",
    "                delta = np.sqrt(delta_eta**2 + delta_phi**2)\n",
    "\n",
    "                kT = min(pt[i], pt[j]) * delta\n",
    "                z = min(pt[i], pt[j]) / (pt[i] + pt[j] + 1e-8)\n",
    "                m2 = (E[i] + E[j])**2 - ((px[i] + px[j])**2 + (py[i] + py[j])**2 + (pz[i] + pz[j])**2)\n",
    "\n",
    "                U[i, j, 0] = float(delta)\n",
    "                U[i, j, 1] = float(kT)\n",
    "                U[i, j, 2] = float(z)\n",
    "                U[i, j, 3] = float(m2)\n",
    "\n",
    "        # ---- build single class label ----\n",
    "        label_vals = [labels[name] for name in self.label_names]\n",
    "\n",
    "        # QCD is float, others are bool/int\n",
    "        if label_vals[0] > 0:\n",
    "            y = 0\n",
    "        else:\n",
    "            y = int(np.argmax(label_vals[1:]) + 1)\n",
    "\n",
    "\n",
    "        return x, U, mask, y\n",
    "    \n",
    "# Training and evaluation functions\n",
    "from itertools import cycle\n",
    "\n",
    "def train_one_iter(model, batch, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    x, U, mask, y = batch\n",
    "    x = x.to(device)\n",
    "    U = U.to(device)\n",
    "    mask = mask.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits = model(x, U, mask)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, U, mask, y in loader:\n",
    "        x = x.to(device)\n",
    "        U = U.to(device)\n",
    "        mask = mask.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(x, U, mask)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.numel()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d1358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0+cu128\n",
      "cuda available: True\n",
      "torch cuda version: 12.8\n",
      "CUDA_VISIBLE_DEVICES: None\n",
      "nvidia-smi failed: Command '['nvidia-smi']' returned non-zero exit status 18.\n"
     ]
    }
   ],
   "source": [
    "# check GPU availability\n",
    "import torch, os, subprocess, textwrap\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"torch cuda version:\", torch.version.cuda)\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "\n",
    "try:\n",
    "    out = subprocess.check_output([\"nvidia-smi\"], stderr=subprocess.STDOUT).decode()\n",
    "    print(out.splitlines()[0])\n",
    "except Exception as e:\n",
    "    print(\"nvidia-smi failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c0036ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z.ling.865/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/z.ling.865/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/_tensor.py:1645: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1939.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NVML_SUCCESS == DriverAPI::get()->nvmlInit_v2_() INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1098, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, total_iters \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     65\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_iter)\n\u001b[0;32m---> 67\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# Evaluation & checkpoint\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 198\u001b[0m, in \u001b[0;36mtrain_one_iter\u001b[0;34m(model, batch, optimizer, device)\u001b[0m\n\u001b[1;32m    194\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    196\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 198\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, y)\n\u001b[1;32m    201\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/smartpixel-brevitas/particle_transformer/model.py:75\u001b[0m, in \u001b[0;36mParTModel.forward\u001b[0;34m(self, x, U, mask)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Particle Attention Blocks\u001b[39;00m\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_proj(x)  \u001b[38;5;66;03m# initial projection\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m U \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteract_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# project interaction embeddings\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpab_num):\n\u001b[1;32m     77\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpab_blocks[_](x, U, mask)\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/smartpixel-brevitas/particle_transformer/layers/dense.py:52\u001b[0m, in \u001b[0;36mQDenseLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Brevitas expects float tensors during QAT; it will insert fake quant.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)\n\u001b[0;32m---> 52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/brevitas/nn/quant_layer.py:53\u001b[0m, in \u001b[0;36mQuantNonLinearActLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_handler(quant_input)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m---> 53\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpack_output(out)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/brevitas/proxy/runtime_quant.py:198\u001b[0m, in \u001b[0;36mActQuantProxyFromInjectorBase.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    196\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_handler(y)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfused_activation_quant_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# If y is an empty QuantTensor, we need to check if this is a passthrough proxy,\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# otherwise return a simple Tensor\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_create_quant_tensor:\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/brevitas/proxy/runtime_quant.py:90\u001b[0m, in \u001b[0;36mFusedActivationQuantProxy.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;129m@brevitas\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_method\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 90\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_quant(x)\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/modules/activation.py:135\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/nn/functional.py:1701\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1699\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1700\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1701\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NVML_SUCCESS == DriverAPI::get()->nvmlInit_v2_() INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1098, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "# Main training cell\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 512\n",
    "total_iters = 1000000\n",
    "eval_interval = 20000\n",
    "initial_lr = 1e-3\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_ds = ParticleDataset(data_dir=\"data/JetClass_Pythia_train_100M_part0\", num_classes=num_classes)\n",
    "val_ds   = ParticleDataset(data_dir=\"data/JetClass_Pythia_val_5M/val_5M\",  num_classes=num_classes)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "model = ParTModel(\n",
    "    in_features=17,\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    num_classes=num_classes,\n",
    "    w_bit_width=8,\n",
    "    a_bit_width=8,\n",
    "    pab_num=8,\n",
    "    cab_num=2,\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "base_optimizer = optim.RAdam(\n",
    "    model.parameters(),\n",
    "    lr=initial_lr,\n",
    "    betas=(0.95, 0.999),\n",
    "    eps=1e-5,\n",
    "    weight_decay=0.0\n",
    ")\n",
    "\n",
    "optimizer = optim.Lookahead(\n",
    "    base_optimizer,\n",
    "    k=6,\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "def lr_lambda(step):\n",
    "    warm = int(0.7 * total_iters)\n",
    "    if step < warm:\n",
    "        return 1.0\n",
    "    decay_steps = (step - warm) // 20000\n",
    "    return 0.99 ** decay_steps  # decays to ~1% by end\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lr_lambda=lr_lambda\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "\n",
    "best_val_acc = 0.0\n",
    "global_step = 0\n",
    "\n",
    "train_iter = cycle(train_loader)\n",
    "\n",
    "for step in range(1, total_iters + 1):\n",
    "    batch = next(train_iter)\n",
    "\n",
    "    loss = train_one_iter(\n",
    "        model=model,\n",
    "        batch=batch,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluation & checkpoint\n",
    "    if step % eval_interval == 0:\n",
    "        val_acc = evaluate(model, val_loader, device)\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        print(\n",
    "            f\"Iter {step:7d} | \"\n",
    "            f\"Loss {loss:.4f} | \"\n",
    "            f\"Val Acc {val_acc:.4f} | \"\n",
    "            f\"LR {lr:.2e}\"\n",
    "        )\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"results/best_part_model.pth\")\n",
    "\n",
    "\n",
    "# Final checkpoint\n",
    "torch.save(model.state_dict(), \"results/part_model_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad16908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "test_ds = ParticleDataset(\n",
    "    data_dir=\"data/test_20M\",\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_acc = evaluate(model, test_loader, device)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
